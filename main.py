# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mq4Mwm6EnJbcquZIRPqq3YLZwHuQdLP-
"""

pip install pyxlsb

Step1: Data Preparation
Import Libraries and Load Data
from google.colab import drive
drive.mount('/content/drive')

!pip install pyxlsb
import pandas as pd
from google.colab import drive
import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# File paths
file_2010_2014 = '/content/drive/MyDrive/hfea-main/after-revision/ar-2010-2014-xlsb.csv'  # CSV
file_2015_2016 = '/content/drive/MyDrive/hfea-main/after-revision/ar-2015-2016-xlsb.xlsb'  # Binary Excel
file_2017_2018 = '/content/drive/MyDrive/hfea-main/after-revision/ar-2017-2018-xlsb.xlsx'  # Normal Excel

# Read CSV file
df_2010_2014 = pd.read_csv(file_2010_2014)

# Read .xlsb file
!pip install pyxlsb
df-2015-2016 = pd.read_excel(file-2015-2016, engine='pyxlsb')

# Read .xlsx file
df-2017-2018 = pd.read_excel(file-2017-2018)
# Correct file paths (no space, proper file names and extensions)
file_2010_2014 = '/content/drive/MyDrive/hfea-main/after-revision/ar-2010-2014.xlsb'
file_2015_2016 = '/content/drive/MyDrive/hfea-main/after-revision/ar-2015-2016.xlsb'
file_2017_2018 = '/content/drive/MyDrive/hfea-main/after-revision/ar-2017-2018.xlsx'

import pandas as pd

# Read binary Excel files (.xlsb) using pyxlsb
df_2010_2014 = pd.read_excel(file_2010_2014, engine='pyxlsb')
df_2015_2016 = pd.read_excel(file_2015_2016, engine='pyxlsb')

# Read regular Excel file (.xlsx)
df_2017_2018 = pd.read_excel(file_2017_2018)
#Handle Missing Values

# copy datasets
df_2010_2014_copy = df_2010_2014.copy()
df_2015_2016_copy = df_2015_2016.copy()
df_2017_2018_copy = df_2017_2018.copy()
# lower all column names which is better for matching columns
df_2010_2014_copy.columns = df_2010_2014_copy.columns.str.lower()
df_2015_2016_copy.columns = df_2015_2016_copy.columns.str.lower()
df_2017_2018_copy.columns = df_2017_2018_copy.columns.str.lower()
# remove spaces all column names which is better for matching columns
df_2010_2014_copy.columns = df_2010_2014_copy.columns.str.strip()
df_2015_2016_copy.columns = df_2015_2016_copy.columns.str.strip()
df_2017_2018_copy.columns = df_2017_2018_copy.columns.str.strip()
# First, reindex all DataFrames to ensure they have the same columns if not done yet
all_columns = df_2010_2014_copy.columns.union(df_2015_2016_copy.columns).union(df_2017_2018_copy.columns)
df_2010_2014_copy = df_2010_2014_copy.reindex(columns=all_columns)
df_2015_2016_copy = df_2015_2016_copy.reindex(columns=all_columns)
df_2017_2018_copy = df_2017_2018_copy.reindex(columns=all_columns)

# Now, concatenate the datasets along the rows (axis=0)
df_combined = pd.concat([df_2010_2014_copy, df_2015_2016_copy, df_2017_2018_copy], ignore_index=True)

# Optionally, you can reset the index if you want a clean index after concatenation
df_combined.reset_index(drop=True, inplace=True)
import pandas as pd

def remove_low_non_null_columns(df, threshold=0.01):

    # Calculate the percentage of non-null values in each column
    non_null_percentage = df.notnull().mean()

    # Identify columns to keep
    columns_to_keep = non_null_percentage[non_null_percentage >= threshold].index

    # Create a new DataFrame with the filtered columns
    df_filtered = df[columns_to_keep]

    return df_filtered
# Enhanced logic for filling NaN values
def fill_missing_values(df):
    # Replace age ranges with median values
    age_mapping = {
        "18-34": 26, "35-37": 36, "38-39": 38.5, "40-42": 41, "45-50": 47.5, '43-44': 43.5, "999": np.nan
    }
    if 'Patient age at treatment' in df.columns:
        df['Patient age at treatment'] = df['Patient age at treatment'].replace(age_mapping)
        df['Patient age at treatment'] = df['Patient age at treatment'].fillna(df['Patient age at treatment'].median())

    # Categorical columns (fill with mode or a specific category if known)
    categorical_columns = [
        "Type of Infertility - Female Primary", "Type of Infertility - Female Secondary",
        "Type of Infertility - Male Primary", "Type of Infertility - Male Secondary",
        "Type of Infertility -Couple Primary", "Type of Infertility -Couple Secondary",
        "Cause  of Infertility - Tubal disease", "Cause of Infertility - Ovulatory Disorder",
        "Cause of Infertility - Male Factor", "Cause of Infertility - Patient Unexplained",
        "Cause of Infertility - Endometriosis", "Cause of Infertility - Cervical factors",
        "Cause of Infertility - Female Factors", "Cause of Infertility - Partner Sperm Concentration",
        "Cause of Infertility - Partner Sperm Morphology",
        "Cause of Infertility - Partner Sperm Motility",
        "Cause of Infertility - Partner Sperm Immunological factors",
        "Main Reason for Producing Embroys Storing Eggs",
        "Type of treatment - IVF or DI", "Specific treatment type", "PGD", "PGD treatment",
        "PGS", "PGS Treatment", "Egg Source", "Sperm From",
        "Elective Single Embryo Transfer", "Fresh Cycle", "Frozen Cycle",
        "Live Birth Occurrence", "Early Outcome"
    ]

    placeholder = "UNKNOWN"
    for col in categorical_columns:
        if col in df.columns:
            df[col].fillna(placeholder, inplace=True)

    # Date columns (fill with a placeholder date or extrapolate if logical)
    date_columns = [
        "Date of Egg Collection", "Date of Egg Thawing", "Date of Egg Mixing",
        "Date of Embryo Thawing", "Date of Embryo Transfer", "Heart One Delivery Date",
        "Heart Two Delivery Date", "Heart Three Delivery Date", "Heart Four Delivery Date"
    ]

    for col in date_columns:
        if col in df.columns:
            # Use the mode of the column if available, otherwise default to a placeholder
            if df[col].mode().size > 0:  # Check if mode exists
                df[col].fillna(df[col].mode()[0], inplace=True)
            else:
                df[col].fillna("1900-01-01", inplace=True)  # Default placeholder

    # Handle special cases if needed (e.g., ranges or specific text)
    range_columns = [
        "Egg Donor Age at Registration", "Sperm Donor Age at Registration",
        "Heart One Birth Weight", "Heart Two Birth Weight", "Heart Three Birth Weight",
        "Heart Four Birth Weight"
    ]
    for col in range_columns:
        if col in df.columns:
            df[col].fillna(placeholder, inplace=True)

    filled_columns = set(categorical_columns) | set(date_columns) | set(range_columns)
    unfilled_columns = set(df.columns).difference(filled_columns)

    for col in unfilled_columns:
        if col in df.columns:
            df[col].fillna(0, inplace=True)

    # Ensure all updates are applied
    return df

import re

# Define a function to clean values
def clean_value(value):
    if isinstance(value, str):  # Check if the value is a string
        # Remove '<', '>', '=' signs and keep the numeric part
        numeric_part = re.sub(r"[<>=]", "", value)
        try:
            return float(numeric_part)  # Convert to float if possible
        except ValueError:
            return value  # Return as-is if conversion fails
    return value  # Return non-strings as-is

# Apply the cleaning function to all columns in the DataFrame
numerical_columns = df_combined.select_dtypes(include=["number"])
for col in numerical_columns:
    df_combined[col] = df_combined[col].apply(clean_value)
df_combined = df_combined.apply(pd.to_numeric, errors='coerce')
# Drop columns that contain only NaN values
df_combined.dropna(axis=1, how="all", inplace=True)

df_combined = remove_low_non_null_columns(df_combined)

df_combined = fill_missing_values(df_combined)

# Fill categorical NaNs with a special placeholder string
placeholder = "UNKNOWN"

# Get categorical columns
categorical_columns = df_combined.select_dtypes(include=["object"]).columns

# Replace missing values with the placeholder in each dataset
for col in categorical_columns:
    df_combined[col] = df_combined[col].fillna(placeholder)

#Encode Categorical Features
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

# Function to encode categorical columns
def encode_columns(df):
    for col in df.columns:
        if df[col].dtype == 'object':
            # Convert all values to strings to ensure uniformity
            df[col] = df[col].astype(str)
            # Fit and transform the column
            df[col] = label_encoder.fit_transform(df[col])
    return df

# Apply the function to each dataframe copy
# df_combined = encode_columns(df_combined)

from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# Function to one-hot encode categorical columns
def one_hot_encode_columns(df):
    categorical_cols = df.select_dtypes(include=['object']).columns  # Select categorical columns
    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)  # One-hot encode with drop_first=True to avoid dummy variable trap
    return df

# Apply the function to the dataframe
df_combined = one_hot_encode_columns(df_combined)

#Split Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler  # Or StandardScaler

target_column = 'live birth occurrence'

# Define features and target
X = df_combined.drop(columns=[target_column])
y = (df_combined[target_column] > 0).astype("int")  # Binary target

# Split the data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=43)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=43)
#Step 2: Feature Selection with PSO
pip install pyswarm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

def fitness_function(selected_features):
    # Use selected features
    selected_columns = [col for col, include in zip(X_train.columns, selected_features) if include]

    if len(selected_columns) == 0:  # Avoid empty feature subset
        return 0

    # Subset the data
    X_train_subset = X_train[selected_columns]
    X_val_subset = X_val[selected_columns]

    # Train Logistic Regression
    model = LogisticRegression()
    model.fit(X_train_subset, y_train)

    # Validate and calculate F1-score
    y_pred = model.predict(X_val_subset)
    f1 = f1_score(y_val, y_pred)

    # Add a penalty for the number of selected features
    penalty_weight = 0.2  # Adjust this weight to control the reduction
    penalty = len(selected_columns) * penalty_weight

    return -(f1 - penalty)  # Negative for minimization
Run PSO
from pyswarm import pso

# Define bounds: each feature can be either 0 (excluded) or 1 (included)
lb = [0] * X_train.shape[1]
ub = [1] * X_train.shape[1]

# Run PSO 0.2
best_features, _ = pso(fitness_function, lb, ub, swarmsize=20, maxiter=100)

# Interpret Results
selected_columns = [col for col, include in zip(X_train.columns, best_features) if include]
print("Reduced feature set:", selected_columns)
print(f"Number of features selected: {len(selected_columns)}")

Build the Transformer-Based Model
def build_transformer_model(input_dim, num_heads=4, ff_dim=128, num_layers=2, dropout_rate=0.3, l2_reg=1e-4):
    inputs = layers.Input(shape=(input_dim,))

    # Positional Encoding (optional, can skip for non-sequential data)
    x = layers.Dense(ff_dim, activation="relu", kernel_regularizer=tf.keras.regularizers.L2(l2_reg))(inputs)
    x = tf.expand_dims(x, axis=1)  # Add sequence dimension

    for _ in range(num_layers):
        # Multi-head attention
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)
        attn_output = layers.Dropout(dropout_rate)(attn_output)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization()(x)

        # Feed-forward network
        ff_output = layers.Dense(ff_dim, activation="relu", kernel_regularizer=tf.keras.regularizers.L2(l2_reg))(x)
        ff_output = layers.Dropout(dropout_rate)(ff_output)
        x = layers.Add()([x, ff_output])
        x = layers.LayerNormalization()(x)

    # Global pooling
    x = layers.GlobalAveragePooling1D()(x)

    # Output layer
    outputs = layers.Dense(1, activation="sigmoid", kernel_regularizer=tf.keras.regularizers.L2(l2_reg))(x)

    model = models.Model(inputs=inputs, outputs=outputs)
    return model
#Prepare data
# Subset the data using selected features
X_train_reduced = X_train[selected_columns]
X_val_reduced = X_val[selected_columns]
X_test_reduced = X_test[selected_columns]

# Initialize the scaler
scaler = MinMaxScaler()

# Batch and shuffle datasets
batch_size = 2048

# Normalize Numerical Features
X_train_scaled = scaler.fit_transform(X_train_reduced)
X_val_scaled = scaler.transform(X_val_reduced)
X_test_scaled = scaler.transform(X_test_reduced)

# convert them to dataframe
X_train_final = pd.DataFrame(X_train_scaled, columns=X_train_reduced.columns)
X_val_final = pd.DataFrame(X_val_scaled, columns=X_val_reduced.columns)
X_test_final = pd.DataFrame(X_test_scaled, columns=X_test_reduced.columns)

# Convert data to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((X_train_final.values, y_train.values))
val_dataset = tf.data.Dataset.from_tensor_slices((X_val_final.values, y_val.values))
test_dataset = tf.data.Dataset.from_tensor_slices((X_test_final.values, y_test.values)).batch(batch_size)

train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)
val_dataset = val_dataset.batch(batch_size)
#Train the Model
# Initialize the model
input_dim = len(selected_columns)  # Use the reduced feature set from PSO
model = build_transformer_model(input_dim)

from sklearn.utils.class_weight import compute_class_weight
# Compute the class weights
class_weights_array = compute_class_weight(class_weight="balanced",
                                           classes=np.unique(y_train),
                                           y=y_train)

# Convert to a dictionary
class_weights = {i: weight for i, weight in enumerate(class_weights_array)}

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001),
              loss="binary_crossentropy",
              metrics=["accuracy", tf.keras.metrics.AUC()])

# Train the Transformer model
history = model.fit(train_dataset,
                    validation_data=val_dataset,
                    epochs=40,
                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)])

import matplotlib.pyplot as plt

# Extract the history of training
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Plot training and validation loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# Evaluate and Optimize
# Evaluate the model on the test set
X_test_reduced = X_test[selected_columns]
test_dataset = tf.data.Dataset.from_tensor_slices((X_test_reduced.values, y_test.values)).batch(batch_size)

test_loss, test_accuracy, test_auc = model.evaluate(test_dataset)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Test AUC: {test_auc}")

# Fine-tune hyperparameters (optional)
# You can use a grid search or additional PSO for hyperparameter tuning

# Adjust classification threshold to balance precision and recall
y_test_pred_prob = model.predict(X_test_reduced)
optimal_threshold = 0.5  # Default threshold, can adjust based on validation PR curve
y_test_pred = (y_test_pred_prob >= optimal_threshold).astype("int")